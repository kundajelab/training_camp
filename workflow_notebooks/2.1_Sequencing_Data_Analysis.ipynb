{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing data analysis\n",
    "\n",
    "### IMPORTANT: Please make sure that your are using the bash kernel to run this notebook.\n",
    "#### (Do this at the beginning of every session) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers analysis of DNA sequencing data from raw files to processed signals.\n",
    "\n",
    "Although this analysis is for ATAC-seq data, many of the steps (especially the first section) are the same for other types of DNA sequencing experiments.\n",
    "\n",
    "We'll be doing the analysis in Bash, which is the standard language for UNIX command-line scripting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in the analysis pipeline that are covered in this notebook are indicated below:\n",
    "![Sequencing Data Analysis 1](images/part1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting up the data\n",
    "\n",
    "We start with raw `.fastq.gz` files, which are provided by the sequencing instrument. For each DNA molecule (read) that was sequenced, they provide the nucleotide sequence, and information about the quality of the signal of that nucleotide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up variables storing the location of our data\n",
    "### The proper way to load your variables is with the ~/.bashrc command, but this is very slow in iPython \n",
    "export SUNETID=\"$(whoami)\"\n",
    "export WORK_DIR=\"/scratch/${SUNETID}\"\n",
    "export DATA_DIR=\"${WORK_DIR}/data\"\n",
    "[[ ! -d ${WORK_DIR}/data ]] && mkdir \"${WORK_DIR}/data\"\n",
    "export SRC_DIR=\"${WORK_DIR}/src\"\n",
    "[[ ! -d ${WORK_DIR}/src ]] && mkdir -p \"${WORK_DIR}/src\"\n",
    "export METADATA_DIR=\"/metadata\"\n",
    "export AGGREGATE_DATA_DIR=\"/data\"\n",
    "export AGGREGATE_ANALYSIS_DIR=\"/outputs\"\n",
    "export YEAST_DIR=\"/saccer3\"\n",
    "export TMP=\"${WORK_DIR}/tmp\"\n",
    "export TEMP=$TMP\n",
    "export TMPDIR=$TMP\n",
    "[[ ! -d ${TMP} ]] && mkdir -p \"${TMP}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check exactly which fastqs we have (we copied these from `$AGGREGATE_DATA_DIR`to your personal `$DATA_DIR` in the last tutorial):\n",
    "\n",
    "(recall that the `ls` command lists the contents of a directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As a sanity check, we can also look at the size and last edited time of some of the fastqs by addind `-lrth` to the `ls` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lrth $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also inspect the format of one of the fastqs. Notice that each read takes up 4 lines:\n",
    "1. the read name\n",
    "2. the read's nucleotide sequence\n",
    "3. a '+' to indicate the record contains another line\n",
    "4. a quality score for each base (a number encoded as a letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcat $(ls $DATA_DIR/*gz | head -n 1) | head -n 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2:ATAC-seq data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ENCODE consortium (https://www.encodeproject.org/) uses a standard ATAC-seq data processing pipeline, which can be downloaded here: https://github.com/ENCODE-DCC/atac-seq-pipeline\n",
    "\n",
    "This pipeline is pre-installed on this computer and can be executed by running the **atac.wdl** script through the caper(https://github.com/ENCODE-DCC/caper) tool.  \n",
    "\n",
    "We have not submitted jobs yet, so the command `caper list` shows that no jobs are running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caper list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the pipeline is highly customizable and all the customizations might seem a bit confusing at first, do not worry -- for our purposes, the default settings will suffice. You will run the pipeline on a single sample (i.e. the two replicates for a given strain/timepoint combination). We construct a json file with the parameters needed to run the pipeline. More information about this json file is available here: https://github.com/ENCODE-DCC/atac-seq-pipeline/blob/master/docs/input.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the ATAC-seq pipeline accepts a json file containing the \n",
    "## input parameters for analysis \n",
    "cat ~/cromwell_input_template.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the placeholders \"REP1_R1_PLACEHOLDER\", \"REP1_R2_PLACEHOLDER\", \"REP2_R1_PLACEHOLDER\", \"REP2_R2_PLACEHOLDER\" with your files. You can do this with the \"sed\" command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export REP1_R1=$DATA_DIR/0min_HOG1_1_R1.fastq.gz\n",
    "export REP1_R2=$DATA_DIR/0min_HOG1_1_R2.fastq.gz\n",
    "export REP2_R1=$DATA_DIR/0min_HOG1_2_R1.fastq.gz\n",
    "export REP2_R2=$DATA_DIR/0min_HOG1_2_R2.fastq.gz\n",
    "export experiment=0min_HOG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/cromwell_input_template.json $WORK_DIR/cromwell_input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed -i \"s|REP1_R1_PLACEHOLDER|$REP1_R1|g\" $WORK_DIR/cromwell_input.json\n",
    "sed -i \"s|REP1_R2_PLACEHOLDER|$REP1_R2|g\" $WORK_DIR/cromwell_input.json\n",
    "sed -i \"s|REP2_R1_PLACEHOLDER|$REP2_R1|g\" $WORK_DIR/cromwell_input.json\n",
    "sed -i \"s|REP2_R2_PLACEHOLDER|$REP2_R2|g\" $WORK_DIR/cromwell_input.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $WORK_DIR/cromwell_input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to submit the json file to the caper server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source activate encode-atac-seq-pipeline\n",
    "caper submit /opt/atac-seq-pipeline/atac.wdl -i $WORK_DIR/cromwell_input.json -s $experiment --ip localhost --port 8000\n",
    "\n",
    "#not a typo, run this command twice to prevent the notebook from printing (base) after each cell. \n",
    "conda deactivate \n",
    "conda deactivate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `caper list` command to check on your submitted job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caper list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "store the id of your workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export caper_id=a88a2a6d-474d-45e8-a923-9d0a0e7bac19 #replace with the id of your workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the status is \"Failed\", you can use the `caper troubleshoot` command to print out the error message for the job. `caper troubleshoot` will also tell you the command that the pipeline is executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caper troubleshoot $caper_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caper will write the pipeline outputs to **/scratch/caper/atac**. Run `ls` on that directory to examine the structure of the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /scratch/caper/atac/$caper_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline may run for an hour or so, so meanwhile, we will learn more about what it's doing under the hood. \n",
    "To check on the progress, you can use the `caper troubleshoot` command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's use the `tree`  command to examine the output directory hash: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree /scratch/caper/atac/$caper_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite a complex directory structure, we can use the croo tool (https://github.com/ENCODE-DCC/croo#installation) to aggregate the pipeline outputs that we care about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croo --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the metadata.json file for the caper run. We can do this with the linux `find` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caper_metadata_json=`find  /scratch/caper/atac/$caper_id -name \"metadata.json\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $caper_metadata_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aggregate the pipeline outputs with croo to the $AGGREGATE_ANALYSIS_DIR/croo/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_croo_dir=$AGGREGATE_ANALYSIS_DIR/croo/$experiment\n",
    "echo $experiment_croo_dir\n",
    "#We already aggregated the samples with croo, so no need to run this command. \n",
    "#croo $caper_metadata_json --out-dir $experiment_croo_dir --out-def-json atac.out_def.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's `ls` thre croo output directory to verify the output file organization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $experiment_croo_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the generated report  file (`croo.report.$caper_id.html`)in the browser -- navigate to the ip address of your machine in the browser and navigate to `$experiment_croo_dir` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Examining the pipeline output\n",
    "\n",
    "The pipeline consists of multiple modules, with output files that include the following: \n",
    "\n",
    "```\n",
    "out                               # root dir. of outputs\n",
    "│\n",
    "├ croo.report.*.html                  #  HTML report│\n",
    "├ alignment                           #  mapped alignments\n",
    "│ ├ Replicate 1                          #   for true replicate 1 \n",
    "│ │\n",
    "│ │ ├ *.bam                       #    raw bam\n",
    "│ │ ├ *.nodup.bam (E)             #    filtered and deduped bam\n",
    "│ │ ├ *.tagAlign.gz               #    tagAlign (bed6) generated from filtered bam\n",
    "│ │ ├ *.tn5.tagAlign.gz           #    TN5 shifted tagAlign for ATAC pipeline (not for DNase pipeline)\n",
    "│ │\n",
    "│ ├ Replicate 2                          #   for true repilicate 2\n",
    "│ ...\n",
    "│ │ ...                           \n",
    "│ └ Pooled replicate            #   for pooled pseudo replicates\n",
    "│   ├ ppr1                        #    for pooled pseudo replicate 1 (rep1-pr1 + rep2-pr1 + ...)\n",
    "│   └ ppr2                        #    for pooled pseudo replicate 2 (rep1-pr2 + rep2-pr2 + ...)\n",
    "│\n",
    "├ peaks                             #  peaks called\n",
    "│ └ macs2                          #   peaks generated by MACS2\n",
    "│   ├ rep1                         #    for replicate 1\n",
    "│   │ ├ *.narrowPeak.gz            #     narrowPeak (p-val threshold = 0.01)\n",
    "│   │ ├ *.filt.narrowPeak.gz (E)   #     blacklist filtered narrowPeak \n",
    "│   │ ├ *.narrowPeak.bb (E)        #     narrowPeak bigBed\n",
    "│   │ ├ *.narrowPeak.hammock.gz    #     narrowPeak track for WashU browser\n",
    "│   │ ├ *.pval0.1.narrowPeak.gz    #     narrowPeak (p-val threshold = 0.1)\n",
    "│   │ └ *.pval0.1.*K.narrowPeak.gz #     narrowPeak (p-val threshold = 0.1) with top *K peaks\n",
    "│   ├ rep2                         #    for replicate 2\n",
    "│   ...\n",
    "│   ├ pseudo_reps                          #   for self pseudo replicates\n",
    "│   ├ pooled_pseudo_reps                   #   for pooled pseudo replicates\n",
    "│   ├ overlap                              #   naive-overlapped peaks\n",
    "│   │ ├ *.naive_overlap.narrowPeak.gz      #     naive-overlapped peak\n",
    "│   │ └ *.naive_overlap.filt.narrowPeak.gz #     naive-overlapped peak after blacklist filtering\n",
    "│   └ idr                           #   IDR thresholded peaks\n",
    "│     ├ true_reps                   #    for replicate 1\n",
    "│     │ ├ *.narrowPeak.gz           #     IDR thresholded narrowPeak\n",
    "│     │ ├ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     │ └ *.12-col.bed.gz           #     IDR thresholded narrowPeak track for WashU browser\n",
    "│     ├ pseudo_reps                 #    for self pseudo replicates\n",
    "│     │ ├ rep1                      #    for replicate 1\n",
    "│     │ ...\n",
    "│     ├ optimal_set                 #    optimal IDR thresholded peaks\n",
    "│     │ └ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     ├ conservative_set            #    optimal IDR thresholded peaks\n",
    "│     │ └ *.filt.narrowPeak.gz (E)  #     IDR thresholded narrowPeak (blacklist filtered)\n",
    "│     ├ pseudo_reps                 #    for self pseudo replicates\n",
    "│     └ pooled_pseudo_reps          #    for pooled pseudo replicate\n",
    "│\n",
    "│   \n",
    "│ \n",
    "├ qc                              #  QC logs\n",
    "│ ├ *IDR_final.qc                 #   Final IDR QC\n",
    "│ ├ rep1                          #   for true replicate 1\n",
    "│ │ ├ *.align.log                 #    Bowtie2 mapping stat log\n",
    "│ │ ├ *.dup.qc                    #    Picard (or sambamba) MarkDuplicate QC log\n",
    "│ │ ├ *.pbc.qc                    #    PBC QC\n",
    "│ │ ├ *.nodup.flagstat.qc         #    Flagstat QC for filtered bam\n",
    "│ │ ├ *M.cc.qc                    #    Cross-correlation analysis score for tagAlign\n",
    "│ │ ├ *M.cc.plot.pdf/png          #    Cross-correlation analysis plot for tagAlign\n",
    "│ │ └ *_qc.html/txt               #    ATAQC report\n",
    "│ ...\n",
    "│\n",
    "├ signal                          #  signal tracks\n",
    "│ ├ macs2                         #   signal tracks generated by MACS2\n",
    "│ │ ├ rep1                        #    for true replicate 1 \n",
    "│ │ │ ├ *.pval.signal.bigwig (E)  #     signal track for p-val\n",
    "│ │ │ └ *.fc.signal.bigwig   (E)  #     signal track for fold change\n",
    "│ ...\n",
    "│ └ pooled_rep                    #   for pooled replicate\n",
    "│ \n",
    "├ report                          # files for HTML report\n",
    "└ meta                            # text files containing md5sum of output files and other metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how well the reads aligned to the reference saccer3 genome. We'd like to see an overall alignment rate >=90% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $experiment_croo_dir/qc/rep2/*align.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating across samples, our observed overall alignment rates from bowtie2 were on the low side: \n",
    "\n",
    "\n",
    "| Sample     | Rep  | ID                                        | Overall alignment Rate |\n",
    "|------------|------|-------------------------------------------|------------------------|\n",
    "| 0min_HOG1  | rep1 | 0min_HOG1_1_R1.merged.align.log   | 58.74%                 |\n",
    "| 0min_HOG1  | rep2 | 0min_HOG1_2_R1.merged.align.log     | 65.05%                 |\n",
    "| 0min_HOT1  | rep1 | 0min_HOT1_1_R1.merged.align.log   | 61.70%                 |\n",
    "| 0min_HOT1  | rep2 | 0min_HOT1_2_R1.merged.align.log   | 51.00%                 |\n",
    "| 0min_MSN1  | rep1 | 0min_MSN1_1_R1.merged.align.log  | 67.29%                 |\n",
    "| 0min_MSN1  | rep2 | 0min_MSN1_2_R1.merged.align.log   | 45.98%                 |\n",
    "| 0min_MSN2  | rep1 | 0min_MSN2_1_R1.merged.align.log     | 62.01%                 |\n",
    "| 0min_MSN2  | rep2 | 0min_MSN2_2_R1.merged.align.log    | 69.57%                 |\n",
    "| 0min_MSN4  | rep1 | 0min_MSN4_1_R1.merged.align.log   | 80.48%                 |\n",
    "| 0min_MSN4  | rep2 | 0min_MSN4_2_R1.merged.align.log     | 63.34%                 |\n",
    "| 0min_SKN7  | rep1 | 0min_SKN7_1_R1.merged.align.log    | 94.59%                 |\n",
    "| 0min_SKN7  | rep2 | 0min_SKN7_2_R1.merged.align.log  | 82.67%                 |\n",
    "| 0min_WT    | rep1 | 0min_WT_1_R1.merged.align.log    | 69.60%                 |\n",
    "| 0min_WT    | rep2 | 0min_WT_2_R1.merged.align.log     | 49.54%                 |\n",
    "| 0min_YAP1  | rep1 | 0min_YAP1_1_R1.merged.align.log  | 68.01%                 |\n",
    "| 0min_YAP1  | rep2 | 0min_YAP1_2_R1.merged.align.log  | 43.15%                 |\n",
    "| 0min_YAP6  | rep1 | 0min_YAP6_1_R1.merged.align.log  | 95.60%                 |\n",
    "| 0min_YAP6  | rep2 | 0min_YAP6_2_R1.merged.align.log    | 41.45%                 |\n",
    "| 0min_YAP7  | rep1 | 0min_YAP7_1_R1.merged.align.log  | 93.73%                 |\n",
    "| 0min_YAP7  | rep2 | 0min_YAP7_2_R1.merged.align.log     | 44.50%                 |\n",
    "| 45min_HOG1 | rep1 | 45min_HOG1_1_R1.merged.align.log | 89.00%                 |\n",
    "| 45min_HOG1 | rep2 | 45min_HOG1_2_R1.merged.align.log | 63.16%                 |\n",
    "| 45min_HOT1 | rep1 | 45min_HOT1_1_R1.merged.align.log    | 53.53%                 |\n",
    "| 45min_HOT1 | rep2 | 45min_HOT1_2_R1.merged.align.log | 93.57%                 |\n",
    "| 45min_MSN1 | rep1 | 45min_MSN1_1_R1.merged.align.log | 62.20%                 |\n",
    "| 45min_MSN1 | rep2 | 45min_MSN1_2_R1.merged.align.log  | 64.73%                 |\n",
    "| 45min_MSN2 | rep1 | 45min_MSN2_1_R1.merged.align.log  | 15.44%                 |\n",
    "| 45min_MSN2 | rep2 | 45min_MSN2_2_R1.merged.align.log    | 67.12%                 |\n",
    "| 45min_MSN4 | rep1 | 45min_MSN4_1_R1.merged.align.log    | 67.52%                 |\n",
    "| 45min_MSN4 | rep2 | 45min_MSN4_2_R1.merged.align.log   | 65.88%                 |\n",
    "| 45min_SKN7 | rep1 | 45min_SKN7_1_R1.merged.align.log | 32.87%                 |\n",
    "| 45min_SKN7 | rep2 | 45min_SKN7_2_R1.merged.align.log | 33.96%                 |\n",
    "| 45min_WT   | rep1 | 45min_WT_1_R1.merged.align.log    | 77.00%                 |\n",
    "| 45min_WT   | rep2 | 45min_WT_2_R1.merged.align.log    | 71.12%                 |\n",
    "| 45min_YAP1 | rep1 | 45min_YAP1_1_R1.merged.align.log  | 69.80%                 |\n",
    "| 45min_YAP1 | rep2 | 45min_YAP1_2_R1.merged.align.log  | 29.80%                 |\n",
    "| 45min_YAP6 | rep1 | 45min_YAP6_1_R1.merged.align.log    | 69.54%                 |\n",
    "| 45min_YAP6 | rep2 | 45min_YAP6_2_R1.merged.align.log | 38.73%                 |\n",
    "| 45min_YAP7 | rep1 | 45min_YAP7_1_R1.merged.align.log   | 74.35%                 |\n",
    "| 45min_YAP7 | rep2 | 45min_YAP7_2_R1.merged.align.log   | 34.40%                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine how many  peaks were called for the sample. We use the *zcat* command to examine the contents of a zipped file. \n",
    "\n",
    "We have two sets of peak calls -- optimal overlap peaks and IDR peaks. \n",
    "\n",
    "* optimal overlap peak calls are generated by overlapping peaks from the replicates. \n",
    "* IDR measures consistency between replicates in high-throughput experiments. Also uses reproducibility in score rankings between peaks in each replicate to determine an optimal cutoff for significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcat $experiment_croo_dir/peak/overlap_reproducibility/optimal_peak.narrowPeak.gz | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcat $experiment_croo_dir/peak/idr_reproducibility/optimal_peak.narrowPeak.gz | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcat $experiment_croo_dir/peak/overlap_reproducibility/optimal_peak.narrowPeak.gz | head -n20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate a fragment length distribution from our filtered bam files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We exract fragment length from column 9 of the filtered bam file in replicates 1 & 2\n",
    "samtools view $experiment_croo_dir/align/rep1/*merged.nodup.bam | cut -f9 > $WORK_DIR/fraglength.1.txt\n",
    "samtools view $experiment_croo_dir/align/rep2/*merged.nodup.bam | cut -f9 > $WORK_DIR/fraglength.2.txt\n",
    "cat $WORK_DIR/fraglength.1.txt $WORK_DIR/fraglength.2.txt > $WORK_DIR/fraglength.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now generate a histogram of the fragment lengths\n",
    "python /opt/get_fragment_length_histogram.py --fraglength_file $WORK_DIR/fraglength.txt --o fraglength.png\n",
    "cp fraglength.png ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fragmentlength](fraglength.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing signal tracks in the WashU and UCSC genome browsers ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline uses the MACS2 peak caller to generate two types of signal tracks across the yeast genome: \n",
    "\n",
    "* P-value Tracks \n",
    "* Fold Change Tracks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $experiment_croo_dir/signal/pooled-rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are in binary format, so we cannot print their contents to the terminal, but a number of genome browser tools have been developed that allow us to visualize their contents.  Two of the most popular of these are\n",
    "\n",
    "* UCSC Genome Browser (https://genome.ucsc.edu/cgi-bin/hgGateway) \n",
    "\n",
    "* WashU Epigenome Browser (https://epigenomegateway.wustl.edu/) \n",
    "\n",
    "Both browsers enable you to upload or link your data for visualization. The most efficient way to do this, is to place your bigwig files on a publically accessible  web server, and to link to them from the browser. \n",
    "\n",
    "Navigate to the `$experiment_croo_dir/signal/pooled-rep` folder in your web browser to get the links for use with the WashU Epigenome Browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the full set of fc or pval bigwigs by following these links: \n",
    "\n",
    "**P-value tracks: http://epigenomegateway.wustl.edu/browser/?bundle=0b4cbb30-e8c9-11ea-85f3-a78449ece41c**\n",
    "\n",
    "\n",
    "**Fold change tracks: http://epigenomegateway.wustl.edu/browser/?bundle=ffcc8710-e8da-11ea-aa00-5de4db3892b9**\n",
    "\n",
    "We will now go step-by-step through the process used to generate this visualization. To begin, point your browser to \n",
    "http://epigenomegateway.wustl.edu/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite inefficient to upload our 40 track files one by one. To visualiza files in bulk, the WashU browser allows you to upload \"datahubs\". A datahub is  a file in the json format, which use a nested syntax to specify attributes about how the files are to be visualized. If you're curious, there's more information about such json \"datahubs\" here: http://washugb.blogspot.com/2012/04/data-hub.html. \n",
    "\n",
    "\n",
    "We have generated datahubs for our fc and pval bigwig files here: \n",
    "\n",
    "http://1.gentc.net/outputs/pval_bigwig.json and\n",
    "\n",
    "http://1.gentc.net/outputs/fc_bigwig.json\n",
    "\n",
    "don't worry about the syntax of these files for now (you can generally copy the syntax of these and just replace your file names and urls). The main point is to be aware that these hubs can be used to group visualizations of multiple browser tracks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other QC metrics \n",
    "\n",
    "The full set of QC metrics for each sample can be accessed here: http://1.gentc.net/outputs/reports/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Creating a merged peak set across all samples for downstream analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the peaks across all conditions to create a master list of peaks for analysis. To do this, we concatenate the IDR peaks from all experiments, sort them, and merge them. \n",
    "\n",
    "We take the output of the processing pipeline from the $AGGREGATE_ANALYSIS directory. This is the same analysis you performed above, but gathered in one location for all experiments conducted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use the \"find\" command to identify all IDR narrowPeak output files and write them to a file. \n",
    "find -L $AGGREGATE_ANALYSIS_DIR/croo  -wholename \"*/peak/overlap_reproducibility/optimal_peak.narrowPeak.gz\" > narrowPeak_files.txt\n",
    "#sanity check the file \n",
    "cat narrowPeak_files.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc -l narrowPeak_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat narrowPeak_files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, iterate through the list of narrowPeak files and concatenate them into a single master peak list. \n",
    "for f in `cat narrowPeak_files.txt`\n",
    "do \n",
    "    zcat $f >> all.peaks.bed\n",
    "done\n",
    "\n",
    "#sanity check the all.peaks.bed file \n",
    "head all.peaks.bed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the concatenated file \n",
    "bedtools sort -i all.peaks.bed > all.peaks.sorted.bed \n",
    "\n",
    "head all.peaks.sorted.bed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the sorted, concatenated fileto join overlapping peaks \n",
    "bedtools merge -i all.peaks.sorted.bed > all_merged.peaks.bed \n",
    "\n",
    "head all_merged.peaks.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we use the awk command to add row numbers to the merged peak file, such that each peak has a unique identifier. \n",
    "\n",
    "#We cannot do this 'in place', so we use an intermediate output file \n",
    "awk  -v OFS='\\t' '{print $0,NR}' all_merged.peaks.bed > o.tmp\n",
    "mv o.tmp all_merged.peaks.bed\n",
    "\n",
    "head all_merged.peaks.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating read count and fold change matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to calculate the signal strength in each sample at the genomic regions in **all_merged.peaks.bed**. As we saw above, the ATAC-seq pipeline generates genome-wide fold change signal tracks for each sample that can be used for this calculation (the \\*fc.bigwig and \\*pval.bigwig files). We use the **bigWigAverageOverBed** utility to computue the mean signal from the pval tracks and the mean signal from the fold change tracks for each genomic region in each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load ucsc_tools\n",
    "bigWigAverageOverBed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we find all the fold change bigWig files\n",
    "cd $WORK_DIR\n",
    "find -L $AGGREGATE_ANALYSIS_DIR/croo  -wholename \"*signal/rep1/*tn5.fc.signal.bigwig\" > all.fc.bigwig\n",
    "find -L $AGGREGATE_ANALYSIS_DIR/croo  -wholename \"*signal/rep2/*tn5.fc.signal.bigwig\" >> all.fc.bigwig\n",
    "\n",
    "head all.fc.bigwig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc -l all.fc.bigwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through all bigWig fold change tracks to compute mean signal strength at each genomic region \n",
    "for f in `cat all.fc.bigwig`\n",
    "do\n",
    "\n",
    "    #we extract the part of the filename that corresponds to the sample name and write it as the header in the fc.signal file\n",
    "    sample_name=`basename $f | awk -F'[.]' '{print $1}'`\n",
    "    echo \"$sample_name\"\n",
    "    echo $sample_name > $sample_name.fc.signal.tmp \n",
    "    \n",
    "    \n",
    "    bigWigAverageOverBed $f all_merged.peaks.bed $sample_name.fc.signal.data.tmp \n",
    "    cut -f5 $sample_name.fc.signal.data.tmp >> $sample_name.fc.signal.tmp\n",
    "\n",
    "    #cleanup the intermediate file \n",
    "    rm $sample_name.fc.signal.data.tmp \n",
    "done\n",
    "paste *fc.signal.tmp > all.fc.txt\n",
    "#cleanup intermediate files that were generated \n",
    "rm *.tmp\n",
    "\n",
    "#examine the output \n",
    "head all.fc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the fold change data matrix, we would also like to know the number of reads that pile up at each peak region. This is useful for determining differential chromatin accessibility across samples. \n",
    "To calculate the read count matrix, we will use the **bedtools coverage** command on the *tagAlign* files generated by the processing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we find all the tagAlign\n",
    "cd $WORK_DIR\n",
    "find -L $AGGREGATE_ANALYSIS_DIR/croo  -wholename \"*align/rep1/*merged.nodup.tn5.tagAlign.gz\" > all.tagAlign.files.txt\n",
    "find -L $AGGREGATE_ANALYSIS_DIR/croo  -wholename \"*align/rep2/*merged.nodup.tn5.tagAlign.gz\" >> all.tagAlign.files.txt\n",
    "\n",
    "head all.tagAlign.files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc -l all.tagAlign.files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how the bedtools coverage command works\n",
    "bedtools coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through all tagAlign files to compute read count at each peak region.  \n",
    "for f in `cat all.tagAlign.files.txt`\n",
    "do\n",
    "    sample_name=`basename $f | awk -F'[.]' '{print $1}'`\n",
    "    echo \"$sample_name\"\n",
    "    echo $sample_name > $sample_name.readcount.tmp \n",
    "    bedtools coverage -counts -a all_merged.peaks.bed -b $f  | cut -f5 >>$sample_name.readcount.tmp \n",
    "done\n",
    "paste *.readcount.tmp > all.readcount.txt\n",
    "#cleanup the temporary files\n",
    "rm *.tmp\n",
    "\n",
    "#examine the output \n",
    "head all.readcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the counts in the first and second columns are on a different scale. This makes sense because if a particular sample had more reads to begin with, the raw counts for each peak will be higher. \n",
    "We can address this problem with sample normalization, covered in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we add in the peak names to our counts file and fold change file so we can keep track of which row \n",
    "#corresponds to which peak. \n",
    "\n",
    "\n",
    "#add a header to the merged peak file \n",
    "sed -i '1i\\Chrom\\tStart\\tEnd\\tID' all_merged.peaks.bed\n",
    "\n",
    "#paste the peak bed file region annotation matrix to the signal matrix\n",
    "paste all_merged.peaks.bed all.fc.txt > o.tmp \n",
    "mv o.tmp all.fc.txt \n",
    "\n",
    "paste all_merged.peaks.bed all.readcount.txt > o.tmp\n",
    "mv o.tmp all.readcount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head all.readcount.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head all.fc.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In examining the files, we notice that all the files end with the suffix \"\\_R1_001\". This is an artifact generated by the processing pipeline. This part of the filename is not informative for our purposes, since it's shared by all samples, so we can remove it with the **sed** command. The syntax is illustrated below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed -i 's/_R1//g' all.fc.txt\n",
    "sed -i 's/_R1//g' all.readcount.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head all.fc.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head all.readcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now generated a read count matrix and a fold change signal peak regions in our dataset. \n",
    "This completes the basic data processing pipeline. \n",
    "Now, on to drawing conclusions about our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
