{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Getting ready to run code on the cluster#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT: Please make sure that your are using the bash kernel to run this notebook.\n",
    "\n",
    "Now that you can submit jobs like any self-respecting Unix ninja, you are ready to start analyzing data! Here you will learn about how to organize your research directory and setup the cluster environment to access all software you wish to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing your research as a pro##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really nice paper with guidelines on organizing computational projects in an organized and snazzy fashion: (http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424). \n",
    "![Analysis Pipeline](images/journal.pcbi.1000424.g001.png)\n",
    "\n",
    "Let's see this in action!\n",
    "\n",
    "\n",
    "First, let's set up our working directory (also known as \"scratch directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export WORKDIR=/scratch/$(whoami)\n",
    "\n",
    "echo $WORKDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize your folder into subdirectories as a pro: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ${WORKDIR}\n",
    "mkdir data src\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $WORKDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run code on the cluster ## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data processing will use multiple software tools. To be able to access them, we can load their paths into our session, by loading their respective modules.\n",
    "\n",
    "To load a module, you can type\n",
    "**module load [desiredModule]** - this is going to modify your path\n",
    "\n",
    "Once a module is loaded, you can use the code associated with that module directly. For instance, let's say you want to load a module for BEDTools (a software package we will be using in this training camp). If you run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source /etc/profile.d/modules.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load bedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It loads the bedtools code, such that when you are ready to use the code, you can just directly call commands. Note that the -h or --help arguments can often be used to give help about a particular tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module unload bedtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load bedtools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which bedtools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry, you do not need to know off the top of your head the names of the modules you want. To see all software modules available on the AWS cluster, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module avail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The .bashrc file (=your friend) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where is .bashrc?\n",
    "#In our home directory\n",
    "cd ~\n",
    "pwd\n",
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But this doesn't show bashrc...\n",
    "ls -ah #this does (shows all hidden files)\n",
    "#.bash_logout automatically runs things when you log out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wouldn't it be nice to have everything ready to run when you log into the cluster?\n",
    "To avoid having to run module load commands every time you log in, you can add these commands to a .bashrc file, located in your home directory. The .bashrc file contains a set of commands that get executed every time you log into the server. In this way, every time you log in, you will be all set to run all code you wish.\n",
    "\n",
    "Note: Technically, the ~/.bashrc file is not what's executed on login; it's ~/.bash_profile, which in turn calls ~/.bashrc. If your .bash_profile does not call .bashrc, put the line source ~/.bashrc in your .bash_profile. The difference between the two files is explained here: http://www.joshstaiger.org/archives/2005/07/bash_profile_vs.html\n",
    "\n",
    "Let's add all our desired module loading commands into a .bashrc file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedtools_load='module load bedtools/2.26.0'\n",
    "#naive thing:\n",
    "#NOTE: ~ is a shortcut for your home directory\n",
    "#echo $bedtools_load >> ~/.bashrc #this might clutter up our bashrc if we run it a bunch of times\n",
    "\n",
    "#only add the module load commands to the ~/.bashrc file if they don't exist in this file already \n",
    "#The || acts like an OR; it executes the command on the right if the command on the left errors out\n",
    "#grep -E acts like search\n",
    "#reminder: \"$bedtoos_load\" decodes to 'module load bedtools/2.17.0'\n",
    "grep -E \"$bedtools_load\" ~/.bashrc || echo $bedtools_load >> ~/.bashrc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining shortcuts in the .bashrc file ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why stop here? You can make all your dreams come true in the .bashrc file!\n",
    "For instance, you can add to the .bashrc file some shortcuts to your directories of interest, which you can then seamlessly use. Add the following to your .bashrc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grep -E \"shortcuts_defined\" ~/.bashrc || \n",
    "(echo '#shortcuts_defined:' >> ~/.bashrc &&\n",
    "\n",
    "echo 'export SUNETID=\"$(whoami)\"' >> ~/.bashrc &&\n",
    "echo 'export WORK_DIR=\"/scratch/${SUNETID}\"' >> ~/.bashrc &&\n",
    "\n",
    "echo 'export DATA_DIR=\"${WORK_DIR}/data\"' >> ~/.bashrc &&\n",
    "echo '[[ ! -d ${WORK_DIR}/data ]] && mkdir \"${WORK_DIR}/data\"' >> ~/.bashrc &&\n",
    "\n",
    "echo 'export SRC_DIR=\"${WORK_DIR}/src\"' >> ~/.bashrc &&\n",
    "echo '[[ ! -d ${WORK_DIR}/src ]] && mkdir -p \"${WORK_DIR}/src\"' >> ~/.bashrc &&\n",
    "\n",
    "echo 'export METADATA_DIR=\"/metadata\"' >> ~/.bashrc &&\n",
    "echo 'export AGGREGATE_DATA_DIR=\"/data\"' >> ~/.bashrc &&\n",
    "echo 'export AGGREGATE_ANALYSIS_DIR=\"/outputs\"' >> ~/.bashrc &&\n",
    "echo 'export YEAST_DIR=\"/saccer3\"' >> ~/.bashrc &&\n",
    "\n",
    "echo 'export TMP=\"${WORK_DIR}/tmp\"' >> ~/.bashrc &&\n",
    "echo 'export TEMP=$TMP' >> ~/.bashrc && \n",
    "echo 'export TMPDIR=$TMP' >> ~/.bashrc && \n",
    "echo '[[ ! -d ${TMP} ]] && mkdir -p \"${TMP}\"' >> ~/.bashrc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\$\\{WORK\\_DIR\\}** is your main work directory\n",
    "\n",
    "**\\$\\{DATA\\_DIR\\}** is your data/ directory -- used for storing the subset of the data you will be working with.  \n",
    "\n",
    "**\\$\\{SRC\\_DIR\\}** is your src/ directory -- used for storing code. \n",
    "\n",
    "**\\$\\{METADATA_DIR}** is the directory with the metadata file for this year's training camp.  \n",
    "\n",
    "\n",
    "**\\$\\{AGGREGATE\\_ANALYSIS\\_DIR}** We will store the aggregate analysis results for all samples in this directory for common use by everyone. \n",
    "\n",
    "**\\$\\{AGGREGATE\\_DATA\\_DIR\\}** is the data/ directory -- this is where we store all the raw data from the sequencer generated by the group\n",
    "\n",
    "**\\$\\{YEAST\\_DIR}** is the directory with the yeast reference genome files \n",
    "\n",
    "**\\$\\{TMP\\_DIR}** is the directory where your temporary files will be stored when you execute code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see your ~/.bashrc and ~/.bash_profile files in action, logout and log in again. All modules should be loaded and all shortcuts should be set!\n",
    "\n",
    "Since logging in/out would disrupt this tutorial, we execute the commands in our ipython notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export SUNETID=\"$(whoami)\"\n",
    "export WORK_DIR=\"/scratch/${SUNETID}\"\n",
    "export DATA_DIR=\"${WORK_DIR}/data\"\n",
    "[[ ! -d ${WORK_DIR}/data ]] && mkdir \"${WORK_DIR}/data\"\n",
    "export SRC_DIR=\"${WORK_DIR}/src\"\n",
    "[[ ! -d ${WORK_DIR}/src ]] && mkdir -p \"${WORK_DIR}/src\"\n",
    "export METADATA_DIR=\"/metadata\"\n",
    "export AGGREGATE_DATA_DIR=\"/data\"\n",
    "export AGGREGATE_ANALYSIS_DIR=\"/outputs\"\n",
    "export YEAST_DIR=\"/saccer3\"\n",
    "export TMP=\"${WORK_DIR}/tmp\"\n",
    "export TEMP=$TMP\n",
    "export TMPDIR=$TMP\n",
    "[[ ! -d ${TMP} ]] && mkdir -p \"${TMP}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and code for this project ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's generally good practice to always keep a backup copy of your raw data files in case you unintentionally delete or modify these files when performing your analysis. \n",
    "\n",
    "For this reason, you will copy the two samples you generated from the **\\$AGGREGATE_DATA_DIR** folder to your personal **\\$DATA_DIR** folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup data files from 2018\n",
    "#cp $AGGREGATE_DATA_DIR/hrosenbl_WT_YPGE_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/pgoddard_asf1_YPGE_1* $DATA_DIR\n",
    "cp $AGGREGATE_DATA_DIR/yiuwong_rtt109_YPGE_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/kjngo_WT_YPGE_2* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/marinovg_asf1_YPGE_2* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/ambenj_rtt109_YPGE_2* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/mkoska_WT_YPGE_3* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/dcotter1_asf1_YPGE_3* $DATA_DIR\n",
    "cp $AGGREGATE_DATA_DIR/jkcheng_rtt109_YPGE_3* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/rpatel7_WT_YPGE_4* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/jarod_asf1_YPGE_5* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/ktomins_rtt109_YPGE_5* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/gamador_WT_YPGE_6* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/raungar_asf1_YPGE_6* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/rosaxma_rtt109_YPD_4* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/dmaghini_WT_YPD_5* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/egreenwa_asf1_YPD_6* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/kjhanson_rtt109_YPD_6* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/hrosenbl_asf1_YPD_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/pgoddard_rtt109_YPD_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/yiuwong_WT_YPD_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/kjngo_rtt109_YPD_2* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/marinovg_WT_YPD_2* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/ambenj_asf1_YPD_2* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/mkoska_asf1_YPD_3* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/dcotter1_rtt109_YPD_3* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/jkcheng_WT_YPD_3* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/rpatel7_asf1_YPGE_4* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/jarod_rtt109_YPGE_4* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/ktomins_WT_YPGE_5* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/gamador_rtt109_YPGE_6* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/raungar_WT_YPD_4* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/rosaxma_asf1_YPD_4* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/dmaghini_asf1_YPD_5* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/egreenwa_rtt109_YPD_5* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/kjhanson_WT_YPD_6* $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data files from 2019 \n",
    "#Note: Copy the 2 replicate for a given strain/timepoint to your data directory. Pick any set of 2 that you like. \n",
    "\n",
    "#srstern\n",
    "#cp $AGGREGATE_DATA_DIR/genegra2_0h_WT_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/srstern_0h_WT_2* $DATA_DIR\n",
    "\n",
    "#ajberg5\n",
    "#cp $AGGREGATE_DATA_DIR/lstrand_4h_WT_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/ajberg5_4h_WT_2* $DATA_DIR\n",
    "\n",
    "#cvduffy\n",
    "#cp $AGGREGATE_DATA_DIR/sierrasb_0h_tf1_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/cvduffy_0h_tf1_2* $DATA_DIR\n",
    "\n",
    "#genegra2\n",
    "#cp $AGGREGATE_DATA_DIR/genegra2_4h_tf1_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/mihayes_4h_tf1_2* $DATA_DIR\n",
    "\n",
    "#subkc\n",
    "#cp $AGGREGATE_DATA_DIR/subkc_0h_tf2_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/annlin_0h_tf2_2* $DATA_DIR\n",
    "\n",
    "#clin5\n",
    "#cp $AGGREGATE_DATA_DIR/srstern_4h_tf2_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/clin5_4h_tf2_2* $DATA_DIR\n",
    "\n",
    "#lstrand\n",
    "#cp $AGGREGATE_DATA_DIR/lstrand_0h_tf3_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/clin5_0h_tf3_2* $DATA_DIR\n",
    "\n",
    "#miao1\n",
    "#cp $AGGREGATE_DATA_DIR/miao1_4h_tf3_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/makena_4h_tf3_2* $DATA_DIR\n",
    "\n",
    "#ajberg5\n",
    "#cp $AGGREGATE_DATA_DIR/ajberg5_0h_tf4_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/miao1_0h_tf4_2* $DATA_DIR\n",
    "\n",
    "#courtrun\n",
    "#cp $AGGREGATE_DATA_DIR/jarhodes_4h_tf4_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/courtrun_4h_tf4_2* $DATA_DIR\n",
    "\n",
    "#jarhodes\n",
    "#cp $AGGREGATE_DATA_DIR/makena_0h_tf5_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/jarhodes_0h_tf5_2* $DATA_DIR\n",
    "\n",
    "#sierrasb\n",
    "#cp $AGGREGATE_DATA_DIR/sierrasb_4h_tf5_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/marinovg_4h_tf5_2* $DATA_DIR\n",
    "\n",
    "#annlin\n",
    "#cp $AGGREGATE_DATA_DIR/annlin_4h_tf9_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/zahoor_4h_tf9_2* $DATA_DIR\n",
    "\n",
    "#myhayes\n",
    "#cp $AGGREGATE_DATA_DIR/myhayes_0h_tf7_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/soumyak_0h_tf7_2* $DATA_DIR\n",
    "\n",
    "\n",
    "#### IF you want to run an additional sample, pick from the ones below, otherwise, TA's will run these #### \n",
    "\n",
    "#annashch\n",
    "#cp $AGGREGATE_DATA_DIR/courtrun_0h_tf6_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/annashch_0h_tf6_2* $DATA_DIR\n",
    "\n",
    "#soumyak\n",
    "#cp $AGGREGATE_DATA_DIR/cvduffy_4h_tf6_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/soumyak_4h_tf6_2* $DATA_DIR\n",
    "\n",
    "#surag\n",
    "#cp $AGGREGATE_DATA_DIR/surag_4h_tf7_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/albalsubr_4h_tf7_2* $DATA_DIR\n",
    "\n",
    "#abalsubr\n",
    "#cp $AGGREGATE_DATA_DIR/abalsubr_0h_tf8_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/zahoor_0h_tf8_2* $DATA_DIR\n",
    "\n",
    "#zahoor\n",
    "#cp $AGGREGATE_DATA_DIR/subkc_4h_tf8_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/annashch_4h_tf8_2* $DATA_DIR\n",
    "\n",
    "#lakss\n",
    "#cp $AGGREGATE_DATA_DIR/marinovg_0h_tf9_1* $DATA_DIR\n",
    "#cp $AGGREGATE_DATA_DIR/surag_0h_tf9_2* $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the analysis begin!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
